# ğŸ§  Disease Risk Prediction from Daily Habits  
**A Data Science Group Project**

[![Streamlit App](https://img.shields.io/badge/ğŸŒ_Live_App-Link-blue?style=flat-square)](https://dailyhabitsdiseasepredictor-ifwfkpix8myuadevbeymnh.streamlit.app/)  
[![GitHub](https://img.shields.io/badge/ğŸ“‚_Repository-Link-green?style=flat-square)](https://github.com/Tiyani-source/daily_habits_disease_predictor.git)  
[![Dataset](https://img.shields.io/badge/ğŸ“Š_Kaggle_Dataset-Link-orange?style=flat-square)](https://www.kaggle.com/datasets/mahdimashayekhi/disease-risk-from-daily-habits)



## ğŸ“˜ Overview
This project investigates whether **daily lifestyle data** can predict chronic disease risk.  
A dataset of **100,000 records (43 features)** from Kaggle was analyzed with feature engineering, model training, and explainability techniques.  

Despite extensive tuning and transformations, the dataset showed **minimal discriminative power (ROCâ€“AUC â‰ˆ 0.5)**, illustrating the **importance of feature quality** in predictive healthcare.

> ğŸ§© *â€œEven the cleanest models canâ€™t fix weakly predictive data.â€*



## ğŸ¯ Objectives
- Clean and preprocess lifestyleâ€“health data for predictive modeling  
- Engineer and select features using statistical and correlation analysis  
- Train ML models and compare performance (Logistic, Tree-based, XGBoost)  
- Deploy the final model via a **Streamlit web app**



## ğŸ§¹ Data Preprocessing Summary
**Dataset:** 100,000 records Ã— 43 features  
**Target Variable:** `target` (Healthy / Diseased)

- ğŸ” **Missing Data:**  
  - Dropped: `alcohol_consumption`, `income`, `gene_marker_flag`  
  - Imputed: numerical columns via *Iterative Imputer (Bayesian Ridge)*  
  - Filled: categorical nulls with â€œUnknownâ€ or â€œNoneâ€
- **Removed**: `income`, `gene_marker_flag`, `alcohol_consumption`, `occupation` (missing or irrelevant)
- **Imputed** missing values using *Iterative Imputer (Bayesian Ridge)*  
- âš™ï¸ **Redundant Features Removed:**  
  - `bmi_scaled`, `bmi_corrected`, `bmi_estimated`, `survey_code`
- **Verified** no major skew â†’ symmetric numeric distributions  
- **Label encoding** for categorical variables
- ğŸ“Š **Normalization & Type Cleaning:**  
  - Verified skewness â‰ˆ 0 â†’ no transformation needed  
- ğŸ” **Correlation Check:**  
  - Dropped `height` & `weight` (high correlation with BMI)

### ğŸ” Correlation Heatmap
Weak featureâ€“target and inter-feature correlations:
![Correlation Heatmap](assets/correlation_heatmap.png)



## ğŸ§© Feature Stratification
Density plots show overlapping distributions between healthy (0) and at-risk (1) classes â€” confirming limited separation.

| Feature | Density Plot |
|----------|--------------|
| **BMI** | ![BMI by target](assets/bmi_by_target.png) |
| **Cholesterol** | ![Cholesterol by target](assets/cholesterol_by_target.png) |
| **Glucose** | ![Glucose by target](assets/glucose_by_target.png) |
| **Stress Level** | ![Stress level by target](assets/stress_by_target.png) |

> Nearly identical curves â†’ low predictive separation.



## ğŸ§® Feature Engineering
Introduced engineered metrics to improve signal:

- ğŸ§¬ **Risk composites**: `metabolic_risk`, `cardio_risk`, `obesity_flag`  
- ğŸ’¡ **Lifestyle indices**: `sleep_efficiency`, `work_life_balance`, `activity_ratio`  
- ğŸ’­ **Categorical encodings**: `stress_cat`, `mental_cat`  
- ğŸ”— **Interactions**: `high_stress_low_support`  
- âš–ï¸ **Ratios**: `waist_height_ratio`, `sugar_ratio`, `water_per_weight`

![Feature Engineering Notebook](assets/feature_engineering.png)



## ğŸ§  Polynomial Feature Interactions
Explored nonlinear relationships (e.g., BMI Ã— Glucose, Stress Ã— Sleep).  
Created polynomial feature expansions using `PolynomialFeatures(degree=2)` with stratified logistic regression.

```python
pipe = Pipeline([
  ("poly", PolynomialFeatures(degree=2, include_bias=False)),
  ("logreg", LogisticRegression(max_iter=5000, class_weight="balanced"))
])
```
ğŸ“Š Result: ROCâ€“AUC = 0.50, confirming minimal nonlinear separation.



âš™ï¸ Model Development

Models Tested
	â€¢	Logistic Regression
	â€¢	Decision Tree
	â€¢	Random Forest
	â€¢	XGBoost (final selected)


â¸»

ğŸ” SHAP Explainability

Feature importance was computed using SHAP to identify key contributors.
Top global SHAP values showed the following influences:

Feature	SHAP Importance
sugar_intake	0.043
bmi	0.042
sleep_hours	0.037
water_intake	0.034
daily_supplement_dosage	0.033


â¸»

ğŸ“‰ Univariate Feature AUC

Each individual featureâ€™s discrimination power was near random.

=== Top 20 Features by Univariate AUC ===
work_hours ........... 0.507  
bmi .................. 0.497  
glucose .............. 0.498  
exercise_type ........ 0.503  
sleep_quality ........ 0.499  
stress_cat ........... 0.499  

ğŸ“ This indicates the target labels are not well explained by available variables.

â¸»

ğŸ§¾ Model Performance Highlights

All Features (XGBoost)

Accuracy : 0.326
Precision: 0.700
Recall   : 0.068
F1-score : 0.124
ROCâ€“AUC  : 0.494

Reduced Feature Set (Recall Optimized)

Accuracy : 0.690
Precision: 0.700
Recall   : 0.975
F1-score : 0.815
ROCâ€“AUC  : 0.497




# ğŸ“‚ File Structure

- ğŸ“ [assets/](assets/) â€” Folder containing visual assets (plots, screenshots)
  - ğŸ–¼ï¸ [correlation_heatmap.png](assets/correlation_heatmap.png)
  - ğŸ–¼ï¸ [bmi_by_target.png](assets/bmi_by_target.png)
  - ğŸ–¼ï¸ [cholesterol_by_target.png](assets/cholesterol_by_target.png)
  - ğŸ–¼ï¸ [glucose_by_target.png](assets/glucose_by_target.png)
  - ğŸ–¼ï¸ [stress_by_target.png](assets/stress_by_target.png)
  - ğŸ–¼ï¸ [shap_summary.png](assets/shap_summary.png)
  - ğŸ–¼ï¸ [feature_engineering.png](assets/feature_engineering.png)
  - ğŸ–¼ï¸ [polynomial_interactions.png](assets/polynomial_interactions.png)
  - ğŸ–¼ï¸ [xgb_output.png](assets/xgb_output.png)

- ğŸ“˜ [README.md](README.md) â€” Project documentation (this file)
- ğŸ§  [daily_habits_disease_prediction.ipynb](daily_habits_disease_prediction.ipynb) â€” Jupyter notebook for EDA, preprocessing, and model training
- ğŸ’» [frontend.py](frontend.py) â€” Streamlit web application
- âš™ï¸ [requirements.txt](requirements.txt) â€” Python dependencies
- ğŸ“Š [selected_features.pkl](selected_features.pkl) â€” Serialized list of top-ranked features
- ğŸ¯ [best_threshold.pkl](best_threshold.pkl) â€” Optimal probability threshold for classification
- ğŸ”® [xgb_model.pkl](xgb_model.pkl) â€” Trained XGBoost model used for predictions


# ğŸ”® Future Work

	â€¢	Integrate clinical and behavioral datasets
	â€¢	Explore multi-label disease categories



# ğŸ§¾ Citation

Gurusinghe, T.M., Senaratna, S.T.S., Jayathilaka, K.A., & Wickramaarachchi, L.T.B. (2025). Disease Risk Prediction from Daily Habits â€“ SLIIT, IT3051: Fundamentals of Data Mining.



## ğŸ‘¥ Collaborators

- [@SenaratnaSTS](https://github.com/ThushanSenaratnaDev)
- [@JayathilakaKA](https://github.com/Kasunianupama)
- [@Tiyani-source](https://github.com/Tiyani-source)
- [@WickramaarachchiLTB](https://github.com/LLWICK)




# ğŸ“˜ Summary

Key takeaway:
Even after feature engineering, SHAP filtering, and polynomial interactions,
the modelâ€™s AUC â‰ˆ 0.5 indicates the dataset itself lacks predictive signal.
Future progress depends on richer, clinically grounded data sources.
